{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIwR8JInyANK"
   },
   "source": [
    "# 1. Backpropogation\n",
    "\n",
    "We will study the backpropagation behavior for an exponential neuron, given by:\n",
    "\n",
    "$$\n",
    "f(z) = e^{2z}-1\n",
    "$$\n",
    "\n",
    "Consider a two-dimensional input given by $x = (x_1, x_2)^T$. A weight vector $w = (w_1, w_2)^T$ and a bias $b$ act on it. Thus, the output of a neuron is given by $f(x_1, x_2) = e^{2(w_1x_1+w_2x_2+b)}-1$.\n",
    "\\\n",
    "\\\n",
    "(a.) Draw the computational graph for the neuron in terms of elementary operations (addition, subtraction, multiplication, division, exponentiation) as seen in class. **[2 points]**\n",
    "\n",
    "(b.) Consider inputs $x_1 = 0.4, x_2=0.5,$ weights $w_1 = 0.2, w_2 = 0.3$ and bias $b = 0.1$. In the same figure, show the values at each node of the graph during forward propagation. **[2 points]**\n",
    "\n",
    "(c.) Use backpropagation to determine the gradients $\\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}, \\frac{\\partial f}{\\partial b}$. Also illustrate in the same figure the intermediate gradients at each node of the computation graph. **[4 points]**\n",
    "\n",
    "(d.) Explain the process of backpropagation you used to compute partial derivatives. **[2 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI7u_38usuUV"
   },
   "outputs": [],
   "source": [
    "# You can insert an image here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HTUBW4dyANM"
   },
   "source": [
    "# 2. Training a small CNN for MNIST digit classification\n",
    "\n",
    "In this problem, you will train a small convolutional neural network for image classification, using PyTorch. We will use the MNIST dataset for digit classification (http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rq_g-n1OyANN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2QYlNmYGyANN",
    "outputId": "4bbb3b56-71e3-4a45-8d35-58c6da6e4178"
   },
   "outputs": [],
   "source": [
    "# Load in the datasets\n",
    "\n",
    "# Download the MNIST Datasets (you will use these variables later on)\n",
    "MNIST_train = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "MNIST_test = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Code adapted from PyTorch https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "train_labels = MNIST_train.targets\n",
    "label = (train_labels == 0).nonzero()\n",
    "for i in range(1, cols * rows + 1):\n",
    "    # Select image of each label\n",
    "    indices = (train_labels == i-1).nonzero()\n",
    "    sample_idx = indices[0,0]\n",
    "    img, label = MNIST_train[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(i-1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image Shape: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZ2p_GPEQbSj",
    "outputId": "32715f6f-2a0e-49b6-efb1-f8af2e71c014"
   },
   "outputs": [],
   "source": [
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQBE1JbPyANP"
   },
   "source": [
    "**[ 3 points ] Define the network structure as follows**\n",
    "\n",
    "* Convolutional layer with 32 kernels, window size 5, padding size 2, stride 1\n",
    "* In place ReLU activation layer\n",
    "* Max pooling layer with window size 2, stride 2\n",
    "* Convolutional layer with 64 kernels, window size 5, padding size 2, stride 1\n",
    "* In place ReLU activation layer\n",
    "* Max pooling layer with window size 2, stride 2\n",
    "* Fully connected layer with 1024 output channels\n",
    "* In place ReLU activation layer\n",
    "* Dropout layer with drop rate 0.4\n",
    "* Fully connected layer with 10 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Te0LYAY4yANQ",
    "outputId": "d96b5672-1e61-4dd7-8f74-9f693b964fbf"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,drop):\n",
    "        super(Net, self).__init__()\n",
    "        # --------------- YOUR CODE HERE ---------------\n",
    "        # DEFINE THE NETWORK STRUCTURE\n",
    "\n",
    "        # Example: self.conv1 = nn.Conv2d(1, 3, 5,stride=1,padding=2,bias=True)\n",
    "        # You can look at the main PyTorch tutorial for reference\n",
    "        # https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --------------- YOUR CODE HERE ---------------\n",
    "        # Example: x = self.conv1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Print net\n",
    "net = Net(drop=True).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_rltLgZyANQ"
   },
   "source": [
    "**[ 5 points ] Complete the train function below. Use the same parameters to perform training in each of the following setups:**\n",
    "\n",
    "* SGD for optimization, without dropout\n",
    "* SGD for optimization, with dropout\n",
    "* Adam for optimization, without dropout\n",
    "* Adam for optimization, with dropout.\n",
    "\n",
    "As evaluation for each case above, perform the following:\n",
    "* Plot the loss graph and the accuracy graph on training set on the same plot\n",
    "* Print the accuracy on test set after training\n",
    "\n",
    "Test accuracies are expected to be quite high (~98 %) for all networks.\n",
    "\n",
    "Training can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN_0N0CDyANR"
   },
   "outputs": [],
   "source": [
    "# CODE BELOW IS AN EXAMPLE STARTER\n",
    "# FEEL FREE TO EDIT ANYTHING\n",
    "\n",
    "# 'to_train' is a parameter that determines what part of the net to train.\n",
    "# It is not required for this question, but will be useful in the next one.\n",
    "# You should also change the parameters: epochs, batch, and learning rate as necessary.\n",
    "# You may need to tune these hyperparameters.\n",
    "def train(train_dataset, net, to_train, opt, epochs=10, batch=200, learning_rate=1e-3):\n",
    "    # Initialize loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losslist = []\n",
    "    acclist=[]\n",
    "\n",
    "    # Create dataloader\n",
    "    MNIST_train_dataloader = DataLoader(train_dataset, batch_size=batch)\n",
    "\n",
    "    # Select optimizer\n",
    "    if(opt=='adam'):\n",
    "        optimizer = optim.Adam(to_train,lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.SGD(to_train,lr=learning_rate,momentum = 0.99)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Set model to training mode\n",
    "    net.train()\n",
    "    for k in tqdm(range(epochs)):\n",
    "        for it, (X,y) in enumerate(MNIST_train_dataloader):\n",
    "            # Send to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Train the model using the optimizer and the batch data\n",
    "            # Append the loss and accuracy to the losslist and acclist arrays\n",
    "            # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "    return losslist,acclist\n",
    "\n",
    "# Used to test or evaluate your network\n",
    "def test(test_dataset, net):\n",
    "    batch = 200\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch)\n",
    "    size = len(test_dataloader.dataset)\n",
    "\n",
    "    # Set model to eval mode\n",
    "    net.eval()\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            # Send to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Prediction\n",
    "            pred = net(X)\n",
    "\n",
    "            # Calculate number of correct predictions in the batch\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # Compute total accuracy\n",
    "    acc = correct / size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "9P0oA7YpyANR",
    "outputId": "8bc5fb15-8c9d-446e-df5b-86a43291a0e8"
   },
   "outputs": [],
   "source": [
    "# SGD with no dropout\n",
    "# Example code\n",
    "net = Net(drop=False).to(device)\n",
    "loss1, acc1 = train(MNIST_train, net, net.parameters(), 'sgd')\n",
    "ax=range(len(loss1))\n",
    "plt.plot(ax, loss1, ax, acc1)\n",
    "plt.legend(['loss', 'accuracy'])\n",
    "plt.show()\n",
    "print('Accuracy:{}'.format(test(MNIST_test, net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbGhjbWFY-zc"
   },
   "outputs": [],
   "source": [
    "# SGD with dropout\n",
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6u7TFzXZAue"
   },
   "outputs": [],
   "source": [
    "# Adam with no dropout\n",
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOb05IZdZBKv"
   },
   "outputs": [],
   "source": [
    "# Adam with dropout\n",
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOjzlyG0yANR"
   },
   "source": [
    "**[ 5 points ] Plot the following graphs and comment on them**\n",
    "\n",
    "* Training loss graphs of SGD−dropout and Adam−dropout on the same plot\n",
    "* Training loss graphs for Adam-dropout for 3 different values of batch size such that there is some difference in the graphs, on the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PWIT9YkyANS"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd0hbsHeyANS"
   },
   "source": [
    "**[ 2 points ] Choose one of the hyperparameters in the train function (examples: learning rate, momentum). Train three models using the same optimizer and using dropout/no dropout for all with different values for the hyperparameter you've chosen. Plot the training loss graphs for the three models on the same plot. Make sure that you change the hyperparameter enough such that there is a clear difference in the graphs and comment on the differences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy7fWmahyANS"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oTW2XulyANU"
   },
   "source": [
    "# 3. Transfer learning\n",
    "\n",
    "You will now visualize the effects of transfer learning by performing experiments using the CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html) . Note that this is just to understand how transfer learning works, in practice it is generally used with very large datasets and complex networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ftR0Fh8ihYs",
    "outputId": "f4db5d6d-e24f-4db6-f3de-76e8047d19d1"
   },
   "outputs": [],
   "source": [
    "# Downloading the dataset\n",
    "!mkdir CIFAR-10\n",
    "%cd CIFAR-10\n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xvzf cifar-10-python.tar.gz\n",
    "!rm cifar-10-python.tar.gz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxajsnSRyANU",
    "outputId": "92cab921-2e7f-4f1a-9b75-a7658a111166"
   },
   "outputs": [],
   "source": [
    "# DATA PARSING\n",
    "path='./CIFAR-10/cifar-10-batches-py/'\n",
    "data=np.zeros((0,32,32,3))\n",
    "labels=[]\n",
    "for i in range(1,6):\n",
    "    with open(path+'data_batch_'+str(i), 'rb') as fo:\n",
    "        dat = pickle.load(fo,encoding='latin1')\n",
    "        r=dat['data'][:,:1024*1].reshape((10000,32,32,1))\n",
    "        g=dat['data'][:,1024:2048].reshape((10000,32,32,1))\n",
    "        b=dat['data'][:,2048:3072].reshape((10000,32,32,1))\n",
    "        rgb=np.concatenate((r,g,b),axis=3)\n",
    "        data=np.vstack((data,np.float32(rgb)/255))\n",
    "        labels+=dat['labels']\n",
    "labels=np.array(labels)\n",
    "# data -> 50000 X 32 X 32 X 3 array with training data\n",
    "# labels -> 50000 labels ranging from 0 to 9\n",
    "print(f\"Data Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14Dx-gBeyANU"
   },
   "source": [
    " **[ 2 points ] Plot 3 random images corresponding to each label from the training data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZCR1MYwyANV"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BB-Wyj5ByANW",
    "outputId": "2d807b10-8d61-406a-cd7a-a7db37967ee7"
   },
   "outputs": [],
   "source": [
    "# Split the data and labels into 2 sets, first one containing labels 0 to 4, and second one from 5 to 9.\n",
    "\n",
    "data1=np.zeros((0,32,32,3))\n",
    "labels1=[]\n",
    "data2=np.zeros((0,32,32,3))\n",
    "labels2=[]\n",
    "for i in range(5):\n",
    "    x=data[labels==i]\n",
    "    data1=np.vstack((data1,x))\n",
    "    labels1+=[i]*len(x)\n",
    "for i in range(5,10):\n",
    "    x=data[labels==i]\n",
    "    data2=np.vstack((data2,x))\n",
    "    labels2+=[i-5]*len(x)\n",
    "\n",
    "labels1=np.array(labels1)\n",
    "labels2=np.array(labels2)\n",
    "\n",
    "torch_data1=data1.transpose((0,3,1,2)) # Switching order of dims to (N, C, H, W)\n",
    "torch_data2=data2.transpose((0,3,1,2)) # Switching order of dims to (N, C, H, W)\n",
    "torch_data1.shape, torch_data2.shape, labels1.shape, labels2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJaTTDaWyANW"
   },
   "source": [
    "**[ 3 points ] Create a simple convolutional network to classify the training data. The network structure should be as follows:**\n",
    "\n",
    "1. Layer 1 - Convolutional layer with kernel size 4, Stride 2, Output channels 5, Relu activation\n",
    "2. Layer 2 - Convolutional layer with kernel size 4, Stride 1, Output channels 10, Relu avtication\n",
    "3. Layer 3 - Convolutional layer with kernel size 4, Stride 1, Output channels 20, Relu activation\n",
    "4. Layer 4 - Convolutional layer with kernel size 4, Stride 1, Output channels 40, Relu activation\n",
    "5. Layer 5 - Fully connected layer with 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0ju7XGiyANW",
    "outputId": "209e0453-e483-404c-d386-c533e337a669"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FJAsD94yANW"
   },
   "source": [
    "**[ 5 points ] Complete the train function below and follow the instructions**\n",
    "\n",
    "* Initialize the network, train the complete network (net.parameters) on data1 (The first 5 classes)\n",
    "* Plot the loss and accuracy graphs over training on the same plot\n",
    "* Print the final training accuracy as well**\n",
    "\n",
    "Set the learning rate, number of iterations and batch size such that the loss is gradually and smoothly decreasing and converging. The accuracy at the end of training must be around or greater than 55 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdxXMhowyANW"
   },
   "outputs": [],
   "source": [
    "# to_train can be net.paramaters OR net.fc.parameters OR net.conv1.parameters so that only certain parts of the net are trained\n",
    "def train(tdata,tlabel,net,to_train):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losslist = []\n",
    "    acclist = [] # Hint: use argmax to find the index with the largest value\n",
    "\n",
    "    # YOU MAY NEED TO CHANGE THESE PARAMETERS TO IMPROVE ACCURACY\n",
    "    epochs=5\n",
    "    batch=100\n",
    "    learning_rate=1e-3\n",
    "    optimizer = optim.Adam(to_train,lr=learning_rate)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for k in tqdm(range(epochs)):\n",
    "        for l in range(int(len(tdata)/batch)):\n",
    "            inds=np.random.randint(0,len(tdata)-1,batch)\n",
    "            inputs = torch.FloatTensor(tdata[inds]).to(device)\n",
    "            inputs.requires_grad_()\n",
    "            targets = torch.LongTensor(tlabel[inds]).to(device)\n",
    "\n",
    "            # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "\n",
    "    return losslist,acclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJ--b_FXyANW"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSNfTOWWyANX"
   },
   "source": [
    "**[ 2 points ] Without reinitializing the network, train only the fully connected layer (net.fc.parameters) now on data2 (The next 5 classes)**\n",
    "\n",
    "Do not change any hyper parameters such as learning rate or batch size. Plot the loss and accuracy and print the final values like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLq1HeGLyANX"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AOR4Z3jyANX"
   },
   "source": [
    "**[ 3 points ] Now repeat the process in the opposite order**\n",
    "\n",
    "* Initialize the net again, train the whole network on data2, generate the same plots as before\n",
    "* Then without reinitializing the net, train only the fully connected layer on data1 and generate the plots\n",
    "\n",
    "Do not change any hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdepBBZeyANX"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVBKoAZSyANX"
   },
   "source": [
    "**[ 5 points ]**\n",
    "\n",
    "* Plot the loss vs iterations for the classifers trained to classify data1, via normal learning as well as transfer learning, on the same plot\n",
    "* Plot another graph for the classifiers trained to classify data2\n",
    "\n",
    "Explain the results obtained, based on the training regimen. Comment on why transfer learning worked/didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "On7RnPNxyANX"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdRNyAh9yANX"
   },
   "source": [
    "Optional: Create a network with more layers, pooling layers, and more filters and try to increase accuracy as much as possible. Play around with the hyperparameters to understand how they affect the training process. No need to turn in anything for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6y7lvyfHrxKI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
